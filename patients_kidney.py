# -*- coding: utf-8 -*-
"""Patients_Kidney.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Evzg4nNKl607QhihnSaTybMJhsYBTqpT

Data Exploration and Cleaning
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('/content/kidney_disease.csv')

# To check First Five Rows
data.head()

# To check Last Five Rows
data.tail()

# Lets cehck basic Information about the dataset
data.info()

# Get summary statistics
print("\nSummary statistics of the dataset:")
print(data.describe(include='all'))

"""Check for Missing Values"""

# Check for missing values
missing_values = data.isnull().sum()
print("\nMissing values in each column:")
print(missing_values)

# Visualize missing values
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title('Heatmap of Missing Values')
plt.show()

"""Data Distribution and Outliers"""

# Check the distribution of numerical variables
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

print("\nDistribution of numerical variables:")
data[numerical_columns].hist(bins=50, figsize=(20, 15))
plt.show()

# Box plots to identify outliers
print("\nBox plots of numerical variables to identify outliers:")
data[numerical_columns].plot(kind='box', subplots=True, layout=(5, 5), figsize=(20, 20), sharex=False, sharey=False)
plt.show()

# Check unique values in categorical variables
categorical_columns = data.select_dtypes(include=['object']).columns

print("\nUnique values in categorical variables:")
for column in categorical_columns:
    print(f"\n{column}:")
    print(data[column].value_counts())

"""Correlation Analysis"""

# Select only numerical columns for correlation analysis
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns

# Correlation matrix for numerical variables
correlation_matrix = data[numerical_columns].corr()
print("\nCorrelation matrix of numerical variables:")
print(correlation_matrix)

# Heatmap of the correlation matrix
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

# Pairplot for key numerical variables
sns.pairplot(data[numerical_columns])
plt.suptitle('Pairplot of Numerical Variables', y=1.02)
plt.show()

# Cross-tabulation for key categorical variables
print("\nCross-tabulation between 'classification' and other categorical variables:")
for column in categorical_columns:
    if column != 'classification':
        print(f"\n{column} vs Classification:")
        print(pd.crosstab(data[column], data['classification']))

"""These cross-tabulations provide valuable insights into the relationships between various categorical variables and the classification of chronic kidney disease (CKD). Let's break down the key findings:
RBC (Red Blood Cells):
Abnormal RBC counts are only present in CKD cases.
Normal RBC counts are seen in both CKD and non-CKD cases, but more frequently in non-CKD.
PC (Pus Cell):
Abnormal PC counts are almost exclusively associated with CKD.
Normal PC counts are seen in both CKD and non-CKD cases.
PCC (Pus Cell Clumps):
Present only in CKD cases.
Absent in both CKD and non-CKD cases.
BA (Bacteria):
Present only in CKD cases.
Absent in both CKD and non-CKD cases.
PCV (Packed Cell Volume):
Lower PCV values (< 40) are predominantly associated with CKD.
Higher PCV values (â‰¥ 40) are more common in non-CKD cases.
WC (White Blood Cells) and RC (Red Blood Cells Count):
Show variations across both CKD and non-CKD cases.
Certain ranges might be more indicative of CKD or non-CKD.
HTN (Hypertension):
Present in all CKD cases.
Absent in all non-CKD cases.
DM (Diabetes Mellitus):
Present in most CKD cases.
Absent in all non-CKD cases.
CAD (Coronary Artery Disease):
Present only in some CKD cases.
Absent in all non-CKD cases.
Appetite:
Poor appetite is only associated with CKD cases.
Good appetite is seen in both CKD and non-CKD cases.
PE (Pedal Edema):
Present only in some CKD cases.
Absent in all non-CKD cases.
ANE (Anemia):
Present only in some CKD cases.
Absent in all non-CKD cases.
Overall, these cross-tabulations suggest that certain factors like abnormal RBC and PC counts, presence of PCC and BA, hypertension, diabetes, poor appetite, pedal edema, and anemia are strongly associated with CKD. The data also indicates that some variables (like PCV, WC, and RC) might have specific ranges that are more indicative of CKD or non-CKD status. This information can be valuable for developing diagnostic criteria and understanding risk factors for chronic kidney disease.
"""

# Handling missing values (example: impute with median for numerical columns)
data_filled = data.copy()
for column in numerical_columns:
    data_filled[column].fillna(data_filled[column].median(), inplace=True)

# Handling missing values (example: impute with median for numerical columns)
data_filled = data.copy()

# Fill missing values for numerical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
data_filled[numerical_columns] = data_filled[numerical_columns].apply(lambda col: col.fillna(col.median()))

# For categorical variables, fill with mode
categorical_columns = data.select_dtypes(include=['object']).columns
data_filled[categorical_columns] = data_filled[categorical_columns].apply(lambda col: col.fillna(col.mode()[0]))

# Verify the cleaning process
print("\nMissing values after handling:")
print(data_filled.isnull().sum())

"""Exploratory Data Analysis"""

# Box plots for numerical variables
print("\nBox plots for numerical variables:")
data.plot(kind='box', subplots=True, layout=(5, 5), figsize=(20, 20), sharex=False, sharey=False)
plt.show()

# Scatter plots for key variables
print("\nScatter plots for key variables:")
key_vars = ['age', 'bp', 'sc']  # Example key variables
sns.pairplot(data[key_vars])
plt.suptitle('Scatter Plots of Key Variables', y=1.02)
plt.show()

# Heatmaps or correlation matrices to show relationships between variables
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
correlation_matrix = data[numerical_columns].corr()
print("\nCorrelation matrix of numerical variables:")
print(correlation_matrix)

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

# Correlation between blood pressure and albumin levels
bp_al_correlation = data[['bp', 'al']].corr()
print("\nCorrelation between blood pressure and albumin levels:")
print(bp_al_correlation)

"""The correlation is positive, meaning that as blood pressure tends to increase, albumin levels also tend to increase slightly, and vice versa.
However, the correlation is weak (close to 0), suggesting that the relationship is not very strong or consistent.
"""

# Analyze how diabetes and hypertension impact kidney health
print("\nImpact of diabetes and hypertension on kidney health:")
sns.boxplot(x='dm', y='bp', data=data)
plt.title('Impact of Diabetes on Blood Pressure')
plt.show()

sns.boxplot(x='htn', y='bp', data=data)
plt.title('Impact of Hypertension on Blood Pressure')
plt.show()

"""Hypertension Group (htn = "yes"):

The median BP is around 85 mmHg.
The middle 50% of individuals have BP between approximately 75 and 95 mmHg.
There are outliers with BP values above 140 mmHg and below 50 mmHg.
Non-Hypertension Group (htn = "no"):

The median BP is around 80 mmHg.
The middle 50% of individuals have BP between approximately 70 and 90 mmHg.
There are outliers with BP values above 120 mmHg and below 55 mmHg.
Overall, the graph suggests that individuals with hypertension tend to have higher BP levels compared to those without hypertension. This is indicated by the slightly higher median and overall spread of BP values in the "yes" group.
"""

sns.boxplot(x='classification', y='bp', data=data)
plt.title('Blood Pressure by CKD Classification')
plt.show()

""" the graph suggests that individuals with CKD tend to have higher BP levels compared to those without CKD. This is indicated by the slightly higher median and overall spread of BP values in the "ckd" and "ckd1" groups."""

sns.boxplot(x='classification', y='sc', data=data)
plt.title('Serum Creatinine by CKD Classification')
plt.show()

sns.boxplot(x='classification', y='age', data=data)
plt.title('Age by CKD Classification')
plt.show()

# Saving visualizations
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.savefig('correlation_matrix_heatmap.png')

# Box plots for impact analysis
plt.figure(figsize=(12, 6))
sns.boxplot(x='dm', y='bp', data=data)
plt.title('Impact of Diabetes on Blood Pressure')
plt.savefig('impact_of_diabetes_on_bp.png')

plt.figure(figsize=(12, 6))
sns.boxplot(x='htn', y='bp', data=data)
plt.title('Impact of Hypertension on Blood Pressure')
plt.savefig('impact_of_htn_on_bp.png')

plt.figure(figsize=(12, 6))
sns.boxplot(x='classification', y='bp', data=data)
plt.title('Blood Pressure by CKD Classification')
plt.savefig('bp_by_ckd_classification.png')

plt.figure(figsize=(12, 6))
sns.boxplot(x='classification', y='sc', data=data)
plt.title('Serum Creatinine by CKD Classification')
plt.savefig('sc_by_ckd_classification.png')

plt.figure(figsize=(12, 6))
sns.boxplot(x='classification', y='age', data=data)
plt.title('Age by CKD Classification')
plt.savefig('age_by_ckd_classification.png')

print("\nEDA report with visualizations and key findings generated and saved.")

"""Winsorization"""

!pip install scipy

import pandas as pd
import numpy as np
from scipy.stats.mstats import winsorize
import matplotlib.pyplot as plt
import seaborn as sns

def winsorize_series(series, limits):
    return winsorize(series, limits=limits)

# Define the limits for winsorization (e.g., 1% on both sides)
limits = (0.01, 0.01)

# Apply winsorization to each numerical column
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
data_winsorized = data.copy()
for column in numerical_columns:
    data_winsorized[column] = winsorize_series(data[column], limits)

# Histograms before Winsorization
print("Histograms before Winsorization:")
data[numerical_columns].hist(bins=50, figsize=(20, 15))
plt.suptitle('Histograms before Winsorization')
plt.show()

# Histograms after Winsorization
print("Histograms after Winsorization:")
data_winsorized[numerical_columns].hist(bins=50, figsize=(20, 15))
plt.suptitle('Histograms after Winsorization')
plt.show()

# Box plots before Winsorization
print("Box plots before Winsorization:")
data[numerical_columns].plot(kind='box', subplots=True, layout=(5, 5), figsize=(20, 20), sharex=False, sharey=False)
plt.suptitle('Box plots before Winsorization')
plt.show()

# Box plots after Winsorization
print("Box plots after Winsorization:")
data_winsorized[numerical_columns].plot(kind='box', subplots=True, layout=(5, 5), figsize=(20, 20), sharex=False, sharey=False)
plt.suptitle('Box plots after Winsorization')
plt.show()

def winsorize_series_custom(series, lower_percentile, upper_percentile):
    lower_limit = np.percentile(series.dropna(), lower_percentile)
    upper_limit = np.percentile(series.dropna(), upper_percentile)
    return np.clip(series, lower_limit, upper_limit)

# Define the custom percentiles for winsorization (15th and 80th)
lower_percentile = 15
upper_percentile = 80

# Apply winsorization to each numerical column
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
data_winsorized = data.copy()
for column in numerical_columns:
    data_winsorized[column] = winsorize_series_custom(data[column], lower_percentile, upper_percentile)

# Histograms before Winsorization
print("Histograms before Winsorization:")
data[numerical_columns].hist(bins=50, figsize=(20, 15))
plt.suptitle('Histograms before Winsorization')
plt.show()

# Histograms after Winsorization
print("Histograms after Winsorization:")
data_winsorized[numerical_columns].hist(bins=50, figsize=(20, 15))
plt.suptitle('Histograms after Winsorization')
plt.show()

# Box plots before Winsorization
print("Box plots before Winsorization:")
data[numerical_columns].plot(kind='box', subplots=True, layout=(5, 5), figsize=(20, 20), sharex=False, sharey=False)
plt.suptitle('Box plots before Winsorization')
plt.show()

# Box plots after Winsorization
print("Box plots after Winsorization:")
data_winsorized[numerical_columns].plot(kind='box', subplots=True, layout=(5, 5), figsize=(20, 20), sharex=False, sharey=False)
plt.suptitle('Box plots after Winsorization')
plt.show()

data.columns

# Save the cleaned data after Winsorization
data_winsorized.to_csv('cleaned_ckd_data_winsorized.csv', index=False)
print("Cleaned data saved as 'cleaned_ckd_data_winsorized.csv'.")

"""Building a Model"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
!pip install streamlit
import streamlit as st

data = pd.read_csv('/content/cleaned_ckd_data_winsorized.csv')

# Define features and target variable
X = data.drop(['id', 'classification'], axis=1)  # Drop 'id' and target column
y = data['classification']  # Target variable

# Step 1: Build Predictive Models
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Initialize the OneHotEncoder
encoder = OneHotEncoder(handle_unknown='ignore')

# Fit and transform the training data
X_train_encoded = encoder.fit_transform(X_train)

# Transform the test data
X_test_encoded = encoder.transform(X_test)

# Convert to dense arrays if needed
X_train_encoded_dense = X_train_encoded.toarray()
X_test_encoded_dense = X_test_encoded.toarray()

# Initialize and train the model
model = LogisticRegression(random_state=42)
model.fit(X_train_encoded_dense, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test_encoded_dense)

# Print the confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Print the classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Print the accuracy score
print("\nAccuracy Score:")
print(accuracy_score(y_test, y_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming X_encoded_dense is your feature matrix and y is your target variable
X_train, X_test, y_train, y_test = train_test_split(X_encoded_dense, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Logistic Regression
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train_scaled, y_train)

# Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

def evaluate_model(model, X_test, y_test, model_name):
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{model_name} Accuracy: {accuracy:.4f}")
    print(f"{model_name} Classification Report:")
    print(classification_report(y_test, y_pred))
    print(f"{model_name} Confusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    print("\n")

# Evaluate models
evaluate_model(lr_model, X_test_scaled, y_test, "Logistic Regression")
evaluate_model(rf_model, X_test_scaled, y_test, "Random Forest")
#evaluate_model(xgb_model, X_test_scaled, y_test, "XGBoost")

data.columns











































from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(handle_unknown='ignore')
X_encoded = encoder.fit_transform(X)

from sklearn.model_selection import train_test_split

# Split the data first
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Then encode only the training data
encoder = OneHotEncoder(handle_unknown='ignore')
X_train_encoded = encoder.fit_transform(X_train)
X_train_encoded_dense = X_train_encoded.toarray()

# Now fit the model
model = LogisticRegression(random_state=42)
model.fit(X_train_encoded_dense, y_train)

print("Shape of X_train_encoded_dense:", X_train_encoded_dense.shape)
print("Shape of y_train:", y_train.shape)

X_test_encoded = encoder.transform(X_test)
X_test_encoded_dense = X_test_encoded.toarray()
y_pred = model.predict(X_test_encoded_dense)

